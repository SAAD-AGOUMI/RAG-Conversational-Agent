# **Agent Conversationnel IA avec Gestion de Documents et RAG**

## **I. Architecture gÃ©nÃ©rale**

Le projet repose sur une architecture modulaire conÃ§ue pour le traitement, lâ€™indexation et lâ€™interrogation de documents via un pipeline de Retrieval-Augmented Generation (RAG).  
Il est structurÃ© autour de trois composants fonctionnels distincts :

- **Chunking** : module chargÃ© de la prÃ©paration des documents, incluant le nettoyage, la normalisation et le dÃ©coupage sÃ©mantique en segments exploitables par les modÃ¨les de langage.
- **Embedding & Retrieval** : composant responsable de la vectorisation des segments, de leur indexation dans une base vectorielle Qdrant, ainsi que de la recherche sÃ©mantique et du re-ranking des rÃ©sultats Ã  lâ€™aide dâ€™un modÃ¨le cross-encoder.
- **Application (App)** : interface utilisateur dÃ©veloppÃ©e avec Streamlit, permettant lâ€™upload de documents, lâ€™interaction avec le systÃ¨me RAG, la gÃ©nÃ©ration de rÃ©ponses et la consultation de lâ€™historique des Ã©changes.

Lâ€™ensemble de lâ€™infrastructure est entiÃ¨rement conteneurisÃ© via Docker, garantissant la reproductibilitÃ© des environnements, lâ€™isolation des services et la facilitÃ© de dÃ©ploiement.

---

## **II. Pour lancer le projet**

### **II.1. DÃ©marrage des conteneurs Docker :**

```bash
sudo docker compose up
```

#### **ğŸ’¡ Remarques :**
- Cette commande dÃ©marre tous les conteneurs et bloque jusqu'Ã  ce que le serveur Ollama ait complÃ¨tement tÃ©lÃ©chargÃ© les modÃ¨les LLM_RAG et LLM_CHUNKING.
- Pendant ce temps, les logs du conteneur Ollama affichent la progression du tÃ©lÃ©chargement des modÃ¨les.
- âš ï¸ **PremiÃ¨re exÃ©cution**â€¯: le premier lancement du chatbot ou de lâ€™agent peut prendre **plus de 10 minutes**, car les modÃ¨les dâ€™Embedding et de rÃ©-ranking doivent Ãªtre tÃ©lÃ©chargÃ©s depuis Hugging Face dans le conteneur. Les exÃ©cutions suivantes seront beaucoup plus rapides.
- Une fois la commande terminÃ©e, tous les services sont opÃ©rationnels et les modÃ¨les sont prÃªts Ã  Ãªtre utilisÃ©s.
- Dans les logs, vous verrez des messages comme :
```bash
pulling manifest
ollama-server  | pulling 2af3b81862c6: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 637 MB
ollama-server  | ...
ollama-server  | verifying sha256 digest
ollama-server  | writing manifest
ollama-server  | success
ollama-server  | Model <LLM_RAG> pulled successfully.
ollama-server  | Model <LLM_CHUNKING> pulled successfully.
```
- Il faut attendre que ce processus soit terminÃ© avant dâ€™utiliser les modÃ¨les ou dâ€™interagir avec lâ€™Agent.

### **II.2. PremiÃ¨re configuration et utilisation**

#### **Ã‰tape 1 : AccÃ©der Ã  l'interface**
- Ouvrez votre navigateur et allez Ã  l'URL suivante : http://localhost:8501
- Attendez pour que l'interface se charge complÃ¨tement.

![image_acceuil](./screenshots/image_acceuil.png)

#### **Ã‰tape 2 : CrÃ©er un utilisateur administrateur**
- Sur la page d'accueil, vous verrez une interface de connexion.
- Cliquez sur "CrÃ©er un compte".
- CrÃ©ez un compte administrateur avec :
    - Nom d'utilisateur admin
    - Mot de passe sÃ©curisÃ©

![image_creation_utilisateur](./screenshots/image_creation_utilisateur.png)

- Pourquoi crÃ©er un admin ? L'administrateur est le seul utilisateur qui peut :
    - Ajouter des documents
    - Lancer le chunking des documents
    - Effectuer l'indexation

#### **Ã‰tape 3 : Se connecter en tant qu'admin**
- Utilisez les identifiants que vous venez de crÃ©er pour vous connecter.

![image_connexion](./screenshots/image_connexion.png)

#### **Ã‰tape 4 : Uploader des documents**
- Dans la barre latÃ©rale (sidebar), cliquez sur "Upload"
- Vous verrez l'interface d'ajout de documents

![image_sidebar_upload](./screenshots/image_sidebar_upload.png)

#### **Ã‰tape 5 : Ajouter des documents**
- Cliquez sur "Browse files"
- SÃ©lectionnez les documents que vous souhaitez ajouter Ã  la base de donnÃ©es
- Une fois les fichiers sÃ©lectionnÃ©s, confirmez l'ajout en cliquant sur le bouton appropriÃ©

#### **Ã‰tape 6 : Lancer le chunking**
- AprÃ¨s confirmation de l'ajout, cliquez sur le bouton : "Lancer le chunking des nouveaux documents"
- Ce processus prÃ©pare les documents pour le traitement

#### **Ã‰tape 7 : RÃ©vectoriser les checks**
- Une fois le chunking terminÃ©, cliquez sur : "Re-vectoriser tous les chunks"
- Cette Ã©tape crÃ©e les embeddings et indexe les documents dans la base vectorielle

#### **Ã‰tape 8 : Utiliser le chatbot**
- Retournez dans le sidebar et cliquez sur "Chatbot"
- Vous pouvez maintenant communiquer avec votre chatbot
- Posez des questions basÃ©es sur les documents que vous avez uploadÃ©s

![image_chatbot](./screenshots/image_chatbot.png)

#### **Ã‰tape 9 (Optionnelle) : CrÃ©er d'autres utilisateurs**
- Vous pouvez vous dÃ©connecter (bouton "DÃ©connexion")
- CrÃ©er un nouveau compte utilisateur standard
- Vous connecter avec ce nouveau compte
- Le chatbot sera Ã©galement accessible pour cet utilisateur, mais sans les privilÃ¨ges d'administration

---

## **III. Installation des dÃ©pendances supplÃ©mentaires**

Cette partie concerne lâ€™installation de dÃ©pendances additionnelles qui ne sont pas requises pour le fonctionnement normal de lâ€™Agent.  
Ces packages sont nÃ©cessaires uniquement pour certains fichiersâ€¯:  
- Le premier script sert Ã  choisir le modÃ¨le dâ€™Embedding Ã  utiliser. (`benchmark_BGE-M3_Multilingual-E5-Large.py`)
- Le deuxiÃ¨me script sert Ã  Ã©valuer nos gÃ©nÃ©rations et rÃ©sultats. (`Evaluation_RAG_Deepeval.py`) 
Ces dÃ©pendances sont donc **optionnelles** et installÃ©es manuellement pour des tests ou Ã©valuations ponctuelles.

### **III.1. Entrer dans le conteneur Agent**

Pour installer ces dÃ©pendances, il faut dâ€™abord accÃ©der au conteneur oÃ¹ tourne lâ€™Agent.  
Cette Ã©tape est nÃ©cessaire car lâ€™installation se fait Ã  lâ€™intÃ©rieur du conteneur et nâ€™interfÃ¨re pas avec lâ€™Agent principal.

```bash
docker exec -it agent /bin/bash
```

### **III.2. Installer les dÃ©pendances**

Une fois dans le conteneur, on installe les packages supplÃ©mentaires listÃ©s dans requirements_additional.txt. Ces packages ne sont pas inclus dans le build Docker par dÃ©faut car ils ne sont pas indispensables au fonctionnement de lâ€™Agent.

```bash
pip install --no-cache-dir -r requirements_additional.txt
```

### **III.3. ExÃ©cution manuelle des scripts Python**

AprÃ¨s installation des dÃ©pendances supplÃ©mentaires, tu peux lancer les scripts spÃ©cifiques Ã  des tests ou Ã©valuationsâ€¯:  

- **Benchmark des modÃ¨les dâ€™Embedding**â€¯: si tu veux tester le choix du modÃ¨le BGE-M3 ou Multilingual-E5-Large, exÃ©cute le script `benchmark_BGE-M3_Multilingual-E5-Large.py`.  
- **Ã‰valuation des gÃ©nÃ©rations RAG avec DeepEval**â€¯: si tu veux Ã©valuer les rÃ©ponses gÃ©nÃ©rÃ©es par lâ€™Agent et obtenir des mÃ©triques, exÃ©cute le script `Evaluation_RAG_Deepeval.py`.  

Ces commandes sont **optionnelles** et ne modifient en rien le fonctionnement normal de lâ€™Agent.

#### **III.3.1. Benchmark des modÃ¨les dâ€™Embedding**

Pour tester diffÃ©rents modÃ¨les dâ€™Embedding (ex. BGE-M3 ou Multilingual-E5-Large) :

```bash
# Depuis le conteneur Agent
python Embedding/Benchmarks/benchmark_BGE-M3_Multilingual-E5-Large.py
```
- Le script va sauvegarder les rÃ©sultats dans `Embedding/Benchmarks/evaluation_results.json`.

#### **III.3.2. Ã‰valuation des gÃ©nÃ©rations RAG avec DeepEval**

##### **a- PrÃ©parer le modÃ¨le LLM pour lâ€™Ã©valuation**

Avant de lancer lâ€™Ã©valuation, assure-toi que le modÃ¨le LLM que tu souhaites utiliser pour DeepEval est installÃ© dans le conteneur Ollama.

- Entrer dans le conteneur Ollama :
```bash
docker exec -it ollama-server /bin/bash
```

- Lister les modÃ¨les existants :
```bash
ollama list
```
VÃ©rifier que le modÃ¨le dÃ©sirÃ© nâ€™est pas dÃ©jÃ  installÃ©.

- Installer le modÃ¨le manuellement si nÃ©cessaire :
```bash
ollama pull NOM_DU_MODELE
```
Remplace NOM_DU_MODELE par le nom exact du modÃ¨le que tu souhaites utiliser.
Lâ€™installation peut prendre plusieurs minutes selon la taille du modÃ¨le.

- VÃ©rifier lâ€™installation :
```bash
ollama list
```
Le modÃ¨le doit maintenant apparaÃ®tre dans la liste.

##### **b- Redirection du conteneur Agent vers Ollama pour DeepEval**

DeepEval ne peut pas contacter Ollama directement Ã  travers les conteneurs, alors il est possible de crÃ©er un forward TCP pour exposer Ollama dans le conteneur Agent.

- Installer socat dans le conteneur Agent
```bash
docker exec -it agent bash
apt-get update && apt-get install -y socat
```

- Lancer le forward TCP
```bash
socat TCP-LISTEN:11434,fork TCP:ollama-server:11434 &
```

- VÃ©rifier la connexion
```bash
python -c "import requests; print(requests.get('http://localhost:11434/v1/models').text)"
```
Si tu obtiens la liste des modÃ¨les Ollama â†’ la redirection fonctionne correctement.

##### **c- Lancer lâ€™Ã©valuation DeepEval**

- Entrer dans le conteneur Agent (si ce nâ€™est pas dÃ©jÃ  fait) :
```bash
docker exec -it agent /bin/bash
```
- ExÃ©cuter le script :
```bash
python Evaluation/RAG/Evaluation_RAG_Deepeval.py
```
- Le script va sauvegarder les rÃ©sultats dans `Evaluation/RAG/evaluation_results.json`.

---

## **IV. Structure du projet**

```text
RAG-Conversational-Agent
â”œâ”€â”€ Agent
â”‚   â”œâ”€â”€ App
â”‚   â”‚   â”œâ”€â”€ config.toml
â”‚   â”‚   â”œâ”€â”€ Home.py
â”‚   â”‚   â”œâ”€â”€ pages
â”‚   â”‚   â”‚   â”œâ”€â”€ Chatbot.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Logs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Settings.py
â”‚   â”‚   â”‚   â””â”€â”€ Upload.py
â”‚   â”‚   â””â”€â”€ utils
â”‚   â”‚       â”œâ”€â”€ assets
â”‚   â”‚       â”‚   â””â”€â”€ images
â”‚   â”‚       â”‚       â””â”€â”€ ineo.jpg
â”‚   â”‚       â”œâ”€â”€ auth_local.py
â”‚   â”‚       â”œâ”€â”€ config_loader.py
â”‚   â”‚       â”œâ”€â”€ data
â”‚   â”‚       â”‚   â””â”€â”€ users
â”‚   â”‚       â”‚       â””â”€â”€ users.json
â”‚   â”‚       â”œâ”€â”€ history_utils.py
â”‚   â”‚       â””â”€â”€ llm_client.py
â”‚   â”œâ”€â”€ Chunking
â”‚   â”‚   â”œâ”€â”€ agentic_chunker_ollama.py
â”‚   â”‚   â”œâ”€â”€ data_chunking
â”‚   â”‚   â”‚   â”œâ”€â”€ chunks.json
â”‚   â”‚   â”‚   â”œâ”€â”€ Documents_traitÃ©s
â”‚   â”‚   â”‚   â”œâ”€â”€ Fichiers_traitÃ©s.json
â”‚   â”‚   â”‚   â”œâ”€â”€ Nouveaux_documents
â”‚   â”‚   â”‚   â””â”€â”€ paragraphs.json
â”‚   â”‚   â”œâ”€â”€ file_readers.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main_chunking.py
â”‚   â”‚   â”œâ”€â”€ process_paragraph.py
â”‚   â”‚   â””â”€â”€ registry.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Embedding
â”‚   â”‚   â”œâ”€â”€ Benchmarks
â”‚   â”‚   â”‚   â”œâ”€â”€ benchmark_BGE-M3_Multilingual-E5-Large.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Benchmark_dataset.json
â”‚   â”‚   â”‚   â””â”€â”€ evaluation_results.json
â”‚   â”‚   â”œâ”€â”€ indexation_database.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ search_and_rerank.py
â”‚   â”œâ”€â”€ Evaluation
â”‚   â”‚   â””â”€â”€ RAG
â”‚   â”‚       â”œâ”€â”€ Evaluation_RAG_Deepeval.py
â”‚   â”‚       â”œâ”€â”€ evaluation_results.json
â”‚   â”‚       â””â”€â”€ Golden_dataset.json
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements_additional.txt
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ utils
â”‚       â””â”€â”€ wait_for_services.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ entrypoint.sh
â””â”€â”€ README.md
```
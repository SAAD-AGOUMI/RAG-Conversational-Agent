# **Agent Conversationnel IA avec Gestion de Documents et RAG**

## **I. Architecture g√©n√©rale**

Le projet repose sur une architecture modulaire con√ßue pour le traitement, l‚Äôindexation et l‚Äôinterrogation de documents via un pipeline de Retrieval-Augmented Generation (RAG).  
Il est structur√© autour de trois composants fonctionnels distincts :

- **Chunking** : module charg√© de la pr√©paration des documents, incluant le nettoyage, la normalisation et le d√©coupage s√©mantique en segments exploitables par les mod√®les de langage.
- **Embedding & Retrieval** : composant responsable de la vectorisation des segments, de leur indexation dans une base vectorielle Qdrant, ainsi que de la recherche s√©mantique et du re-ranking des r√©sultats √† l‚Äôaide d‚Äôun mod√®le cross-encoder.
- **Application (App)** : interface utilisateur d√©velopp√©e avec Streamlit, permettant l‚Äôupload de documents, l‚Äôinteraction avec le syst√®me RAG, la g√©n√©ration de r√©ponses et la consultation de l‚Äôhistorique des √©changes.

L‚Äôensemble de l‚Äôinfrastructure est enti√®rement conteneuris√© via Docker, garantissant la reproductibilit√© des environnements, l‚Äôisolation des services et la facilit√© de d√©ploiement.

---

## **II. Pour lancer le projet**

### **II.1. D√©marrage des conteneurs Docker :**

```bash
sudo docker compose up
```

#### **üí° Remarques :**
- Cette commande d√©marre tous les conteneurs et bloque jusqu'√† ce que le serveur Ollama ait compl√®tement t√©l√©charg√© les mod√®les LLM_RAG et LLM_CHUNKING.
- Pendant ce temps, les logs du conteneur Ollama affichent la progression du t√©l√©chargement des mod√®les.
- ‚ö†Ô∏è **Premi√®re ex√©cution**‚ÄØ: le premier lancement du chatbot ou de l‚Äôagent peut prendre **plus de 10 minutes**, car les mod√®les d‚ÄôEmbedding et de r√©-ranking doivent √™tre t√©l√©charg√©s depuis Hugging Face dans le conteneur. Les ex√©cutions suivantes seront beaucoup plus rapides.
- Une fois la commande termin√©e, tous les services sont op√©rationnels et les mod√®les sont pr√™ts √† √™tre utilis√©s.
- Dans les logs, vous verrez des messages comme :
```bash
pulling manifest
ollama-server  | pulling 2af3b81862c6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 637 MB
ollama-server  | ...
ollama-server  | verifying sha256 digest
ollama-server  | writing manifest
ollama-server  | success
ollama-server  | Model <LLM_RAG> pulled successfully.
ollama-server  | Model <LLM_CHUNKING> pulled successfully.
```
- Il faut attendre que ce processus soit termin√© avant d‚Äôutiliser les mod√®les ou d‚Äôinteragir avec l‚ÄôAgent.

### **II.2. Premi√®re configuration et utilisation**

#### **√âtape 1 : Acc√©der √† l'interface**
- Ouvrez votre navigateur et allez √† l'URL suivante : http://localhost:8501
- Attendez pour que l'interface se charge compl√®tement.

![image_acceuil](./Screenshots/image_acceuil.png)

#### **√âtape 2 : Cr√©er un utilisateur administrateur**
- Sur la page d'accueil, vous verrez une interface de connexion.
- Cliquez sur "Cr√©er un compte".
- Cr√©ez un compte administrateur avec :
    - Nom d'utilisateur admin
    - Mot de passe s√©curis√©

![image_creation_utilisateur](./Screenshots/image_creation_utilisateur.png)

- Pourquoi cr√©er un admin ? L'administrateur est le seul utilisateur qui peut :
    - Ajouter des documents
    - Lancer le chunking des documents
    - Effectuer l'indexation

#### **√âtape 3 : Se connecter en tant qu'admin**
- Utilisez les identifiants que vous venez de cr√©er pour vous connecter.

![image_connexion](./Screenshots/image_connexion.png)

#### **√âtape 4 : Uploader des documents**
- Dans la barre lat√©rale (sidebar), cliquez sur "Upload"
- Vous verrez l'interface d'ajout de documents

![image_sidebar_upload](./Screenshots/image_sidebar_upload.png)

#### **√âtape 5 : Ajouter des documents**
- Cliquez sur "Browse files"
- S√©lectionnez les documents que vous souhaitez ajouter √† la base de donn√©es
- Une fois les fichiers s√©lectionn√©s, confirmez l'ajout en cliquant sur le bouton appropri√©

#### **√âtape 6 : Lancer le chunking**
- Apr√®s confirmation de l'ajout, cliquez sur le bouton : "Lancer le chunking des nouveaux documents"
- Ce processus pr√©pare les documents pour le traitement

#### **√âtape 7 : R√©vectoriser les checks**
- Une fois le chunking termin√©, cliquez sur : "Re-vectoriser tous les chunks"
- Cette √©tape cr√©e les embeddings et indexe les documents dans la base vectorielle

#### **√âtape 8 : Utiliser le chatbot**
- Retournez dans le sidebar et cliquez sur "Chatbot"
- Vous pouvez maintenant communiquer avec votre chatbot
- Posez des questions bas√©es sur les documents que vous avez upload√©s

![image_chatbot](./Screenshots/image_chatbot.png)

#### **√âtape 9 (Optionnelle) : Cr√©er d'autres utilisateurs**
- Vous pouvez vous d√©connecter (bouton "D√©connexion")
- Cr√©er un nouveau compte utilisateur standard
- Vous connecter avec ce nouveau compte
- Le chatbot sera √©galement accessible pour cet utilisateur, mais sans les privil√®ges d'administration

---

## **III. Installation des d√©pendances suppl√©mentaires**

Cette partie concerne l‚Äôinstallation de d√©pendances additionnelles qui ne sont pas requises pour le fonctionnement normal de l‚ÄôAgent.  
Ces packages sont n√©cessaires uniquement pour certains fichiers‚ÄØ:  
- Le premier script sert √† choisir le mod√®le d‚ÄôEmbedding √† utiliser. (`benchmark_BGE-M3_Multilingual-E5-Large.py`)
- Le deuxi√®me script sert √† √©valuer nos g√©n√©rations et r√©sultats. (`Evaluation_RAG_Deepeval.py`) 
Ces d√©pendances sont donc **optionnelles** et install√©es manuellement pour des tests ou √©valuations ponctuelles.

### **III.1. Entrer dans le conteneur Agent**

Pour installer ces d√©pendances, il faut d‚Äôabord acc√©der au conteneur o√π tourne l‚ÄôAgent.  
Cette √©tape est n√©cessaire car l‚Äôinstallation se fait √† l‚Äôint√©rieur du conteneur et n‚Äôinterf√®re pas avec l‚ÄôAgent principal.

```bash
docker exec -it agent /bin/bash
```

### **III.2. Installer les d√©pendances**

Une fois dans le conteneur, on installe les packages suppl√©mentaires list√©s dans requirements_additional.txt. Ces packages ne sont pas inclus dans le build Docker par d√©faut car ils ne sont pas indispensables au fonctionnement de l‚ÄôAgent.

```bash
pip install --no-cache-dir -r requirements_additional.txt
```

### **III.3. Ex√©cution manuelle des scripts Python**

Apr√®s installation des d√©pendances suppl√©mentaires, tu peux lancer les scripts sp√©cifiques √† des tests ou √©valuations‚ÄØ:  

- **Benchmark des mod√®les d‚ÄôEmbedding**‚ÄØ: si tu veux tester le choix du mod√®le BGE-M3 ou Multilingual-E5-Large, ex√©cute le script `benchmark_BGE-M3_Multilingual-E5-Large.py`.  
- **√âvaluation des g√©n√©rations RAG avec DeepEval**‚ÄØ: si tu veux √©valuer les r√©ponses g√©n√©r√©es par l‚ÄôAgent et obtenir des m√©triques, ex√©cute le script `Evaluation_RAG_Deepeval.py`.  

Ces commandes sont **optionnelles** et ne modifient en rien le fonctionnement normal de l‚ÄôAgent.

#### **III.3.1. Benchmark des mod√®les d‚ÄôEmbedding**

Pour tester diff√©rents mod√®les d‚ÄôEmbedding (ex. BGE-M3 ou Multilingual-E5-Large) :

```bash
# Depuis le conteneur Agent
python Embedding/Benchmarks/benchmark_BGE-M3_Multilingual-E5-Large.py
```
- Le script va sauvegarder les r√©sultats dans `Embedding/Benchmarks/evaluation_results.json`.

#### **III.3.2. √âvaluation des g√©n√©rations RAG avec DeepEval**

##### **a- Pr√©parer le mod√®le LLM pour l‚Äô√©valuation**

Avant de lancer l‚Äô√©valuation, assure-toi que le mod√®le LLM que tu souhaites utiliser pour DeepEval est install√© dans le conteneur Ollama.

- Entrer dans le conteneur Ollama :
```bash
docker exec -it ollama-server /bin/bash
```

- Lister les mod√®les existants :
```bash
ollama list
```
V√©rifier que le mod√®le d√©sir√© n‚Äôest pas d√©j√† install√©.

- Installer le mod√®le manuellement si n√©cessaire :
```bash
ollama pull NOM_DU_MODELE
```
Remplace NOM_DU_MODELE par le nom exact du mod√®le que tu souhaites utiliser.
L‚Äôinstallation peut prendre plusieurs minutes selon la taille du mod√®le.

- V√©rifier l‚Äôinstallation :
```bash
ollama list
```
Le mod√®le doit maintenant appara√Ætre dans la liste.

##### **b- Redirection du conteneur Agent vers Ollama pour DeepEval**

DeepEval ne peut pas contacter Ollama directement √† travers les conteneurs, alors il est possible de cr√©er un forward TCP pour exposer Ollama dans le conteneur Agent.

- Installer socat dans le conteneur Agent
```bash
docker exec -it agent bash
apt-get update && apt-get install -y socat
```

- Lancer le forward TCP
```bash
socat TCP-LISTEN:11434,fork TCP:ollama-server:11434 &
```

- V√©rifier la connexion
```bash
python -c "import requests; print(requests.get('http://localhost:11434/v1/models').text)"
```
Si tu obtiens la liste des mod√®les Ollama ‚Üí la redirection fonctionne correctement.

##### **c- Lancer l‚Äô√©valuation DeepEval**

- Entrer dans le conteneur Agent (si ce n‚Äôest pas d√©j√† fait) :
```bash
docker exec -it agent /bin/bash
```
- Ex√©cuter le script :
```bash
python Evaluation/RAG/Evaluation_RAG_Deepeval.py
```
- Le script va sauvegarder les r√©sultats dans `Evaluation/RAG/evaluation_results.json`.
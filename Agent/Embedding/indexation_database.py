"""
üìö Indexation de chunks de documents dans Qdrant avec embeddings vectoriels

Ce script :
- Charge un fichier `chunks.json` contenant des segments (chunks) de documents
- V√©rifie l'int√©grit√© et la structure des donn√©es attendues
- G√©n√®re des embeddings vectoriels √† l'aide du mod√®le EMBEDDING_MODEL
- (Re)cr√©e une collection Qdrant d√©di√©e aux chunks
- Ins√®re chaque chunk dans Qdrant avec ses m√©tadonn√©es associ√©es

Objectif :
Permettre une recherche s√©mantique efficace sur des documents d√©coup√©s
via une base vectorielle (Qdrant).
"""

import os
import uuid
from pathlib import Path

import pandas as pd
import torch
from dotenv import load_dotenv
from qdrant_client import QdrantClient, models
from qdrant_client.http.exceptions import UnexpectedResponse
from sentence_transformers import SentenceTransformer

# D√©termination du dossier racine du projet (2 niveaux au-dessus du fichier courant)
ROOT_DIR = Path(__file__).resolve().parents[1]

# Chemin vers le fichier JSON contenant les chunks
CHUNKS_PATH = ROOT_DIR / "Chunking/data_chunking/chunks.json"

# V√©rification de l'existence du fichier chunks.json
if not CHUNKS_PATH.exists():
    raise FileNotFoundError(f"‚ùå chunks.json introuvable : {CHUNKS_PATH}")

# Chargement des donn√©es JSON dans un DataFrame pandas
df = pd.read_json(CHUNKS_PATH)

# Colonnes obligatoires attendues dans chunks.json
expected_cols = {"text", "document_name", "page_number", "parent_paragraph_id"}
missing = expected_cols - set(df.columns)

# V√©rification de la pr√©sence de toutes les colonnes requises
if missing:
    raise ValueError(f"‚ö†Ô∏è Colonnes manquantes dans chunks.json : {missing}")

# Affichage d'un r√©sum√© du chargement
print(f"‚úÖ {len(df)} chunks charg√©s depuis {CHUNKS_PATH.name}")
print(df.head(3))


# Normalisation et renommage des colonnes utilis√©es par la suite
df["Chunk"] = df["text"].astype(str)
df["Doc"] = df["document_name"]
df["Page"] = df["page_number"].astype(int)
df["Parent"] = df["parent_paragraph_id"]

# D√©tection automatique du p√©riph√©rique
device = "cuda" if torch.cuda.is_available() else "cpu"

# Chargement du mod√®le d'embeddings
load_dotenv()
embedding_model = os.getenv("EMBEDDING_MODEL")
print(f"üß† Chargement du mod√®le {embedding_model}...")
model = SentenceTransformer(embedding_model, device=device)

# Connexion au serveur Qdrant local
client = QdrantClient(host="qdrant-server", port=6333)
collection_name = "documents_chunks"

# Cr√©er la collection uniquement si elle n‚Äôexiste pas
try:
    client.get_collection(collection_name)
except UnexpectedResponse:
    client.create_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(
            size=1024,  # Dimension des embeddings du mod√®le d'embedding
            distance=models.Distance.COSINE,  # Mesure de similarit√©
        ),
    )

# Liste qui contiendra tous les points √† ins√©rer
points = []

# Parcours de chaque chunk pour encodage et pr√©paration des points
for idx, row in df.iterrows():

    # G√©n√©ration d'un ID unique pour chaque chunk
    unique_id = str(
        uuid.uuid5(
            uuid.NAMESPACE_DNS,
            f"{row['Doc']}|{row['Page']}|{row['Parent']}|{row['Chunk']}",
        )
    )

    # G√©n√©ration de l'embedding du chunk
    emb = model.encode(row["Chunk"]).tolist()

    # Cr√©ation du point Qdrant avec vecteur + m√©tadonn√©es
    point = models.PointStruct(
        id=unique_id,
        vector=emb,
        payload={
            "Chunk": row["Chunk"],
            "Doc": row["Doc"],
            "Page": row["Page"],
            "ParentID": row["Parent"],
        },
    )
    points.append(point)

    # Affichage de la progression tous les 50 chunks
    if (idx + 1) % 50 == 0 or idx == len(df) - 1:
        print(f"‚Üí Encod√© {idx + 1}/{len(df)} chunks")

# Insertion finale des points dans la collection Qdrant
print("üöÄ Insertion dans Qdrant...")
client.upsert(collection_name=collection_name, points=points)

# Confirmation de fin de traitement
print("‚úÖ Tous les chunks ont √©t√© ins√©r√©s avec succ√®s dans Qdrant")

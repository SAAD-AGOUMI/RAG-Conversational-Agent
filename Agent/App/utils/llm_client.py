"""
üåê Gestion de la connexion √† Ollama et requ√™tes LLM

Ce module fournit :
1. La configuration du client Ollama pour un environnement conteneuris√©
2. La d√©finition de la variable d'environnement appropri√©e pour le client Ollama
3. Une fonction pour interroger le mod√®le LLM avec un prompt utilisateur
4. Un message syst√®me s√©curisant pour √©viter les hallucinations et prot√©ger
la vie priv√©e
"""

import os
from typing import Optional

import ollama
from dotenv import load_dotenv

# -----------------------------
# üåê Configuration du client Ollama pour conteneur
# -----------------------------
OLLAMA_URL = os.getenv("OLLAMA_URL")

# Cr√©e un client Ollama explicitement li√© au host correct
client = ollama.Client(host=OLLAMA_URL)

# Charger la configuration depuis le fichier .env
load_dotenv()

# -----------------------------
# üõ°Ô∏è Message syst√®me pour √©viter les hallucinations et prot√©ger la vie priv√©e
# -----------------------------
SYSTEM_PROMPT = (
    "R√îLE\n"
    "Tu es un assistant factuel et non sp√©culatif.\n\n"
    "R√àGLES G√âN√âRALES\n"
    "- R√©ponds uniquement √† partir des informations explicitement fournies par"
    "l'utilisateur ou par le contexte du prompt.\n"
    "- N'invente aucune information.\n"
    "- N'ajoute aucun d√©tail qui ne figure pas dans les donn√©es fournies.\n"
    "- Si l'information demand√©e n'est pas pr√©sente, dis-le clairement.\n\n"
    "CONFIDENTIALIT√â\n"
    "- Tu ne fais aucune r√©f√©rence √† des utilisateurs, conversations ou donn√©es non"
    "pr√©sentes dans le prompt.\n"
    "- Si une question porte sur des donn√©es non fournies, r√©ponds que l'information"
    "n'est pas disponible.\n\n"
    "STYLE DE R√âPONSE\n"
    "- R√©ponse directe et concise.\n"
    "- Pas d'explication sur ton fonctionnement interne.\n"
    "- Pas de mention de r√®gles ou de politiques."
)


# -----------------------------
# üí¨ Fonction pour interroger le mod√®le LLM
# -----------------------------
def query_llm(prompt: str, history: Optional[list] = None):
    """
    Envoie un prompt au LLM Ollama et retourne la r√©ponse g√©n√©r√©e.

    Param√®tres :
    - prompt (str) : message utilisateur √† envoyer au mod√®le.
    - history (list | None) : historique optionnel de la conversation,
    sous la forme d'une liste de tuples (role, content, timestamp).
    Si None ou vide, le mod√®le est interrog√© sans contexte conversationnel.

    Retour :
    - str : r√©ponse g√©n√©r√©e par le mod√®le ou message d'erreur en cas d'√©chec.
    """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    if history:
        for role, content, _ in history:
            messages.append({"role": role, "content": content})

    messages.append({"role": "user", "content": prompt})

    model_name = os.getenv("LLM_RAG")
    if model_name is None:
        raise RuntimeError("LLM_RAG is not set")

    try:
        response = client.chat(
            model=model_name,
            messages=messages,
            stream=False,
        )
        return response["message"]["content"]
    except Exception as e:
        return (
            f"‚ö†Ô∏è Erreur de connexion au mod√®le ({model_name}).\n"
            f"Probl√®me avec Ollama.\n\n"
            f"D√©tails : {e}"
        )

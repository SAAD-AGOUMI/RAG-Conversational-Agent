"""
üåê Gestion de la connexion √† Ollama et requ√™tes LLM

Ce module fournit :
1. La configuration du client Ollama pour un environnement conteneuris√© (host = localhost)
2. La d√©finition de la variable d'environnement appropri√©e pour le client Ollama
3. Une fonction pour interroger le mod√®le LLM avec un prompt utilisateur
4. Un message syst√®me s√©curisant pour √©viter les hallucinations et prot√©ger la vie priv√©e
"""

from dotenv import load_dotenv
import os
import ollama

# -----------------------------
# üåê Configuration du client Ollama pour conteneur
# -----------------------------
OLLAMA_HOST = os.getenv("OLLAMA_URL")

# Cr√©e un client Ollama explicitement li√© au host correct
client = ollama.Client(host=OLLAMA_HOST)

# Charger la configuration depuis le fichier .env
load_dotenv()

# -----------------------------
# üõ°Ô∏è Message syst√®me pour √©viter les hallucinations et prot√©ger la vie priv√©e
# -----------------------------
SYSTEM_PROMPT = (
    "R√îLE\n"
    "Tu es un assistant factuel et non sp√©culatif.\n\n"

    "R√àGLES G√âN√âRALES\n"
    "- R√©ponds uniquement √† partir des informations explicitement fournies par l'utilisateur ou par le contexte du prompt.\n"
    "- N'invente aucune information.\n"
    "- N'ajoute aucun d√©tail qui ne figure pas dans les donn√©es fournies.\n"
    "- Si l'information demand√©e n'est pas pr√©sente, dis-le clairement.\n\n"

    "CONFIDENTIALIT√â\n"
    "- Tu ne fais aucune r√©f√©rence √† des utilisateurs, conversations ou donn√©es non pr√©sentes dans le prompt.\n"
    "- Si une question porte sur des donn√©es non fournies, r√©ponds que l'information n'est pas disponible.\n\n"

    "STYLE DE R√âPONSE\n"
    "- R√©ponse directe et concise.\n"
    "- Pas d'explication sur ton fonctionnement interne.\n"
    "- Pas de mention de r√®gles ou de politiques."
)

# -----------------------------
# üí¨ Fonction pour interroger le mod√®le LLM
# -----------------------------
def query_llm(prompt: str):
    """
    Envoie un prompt utilisateur au LLM Ollama et retourne la r√©ponse.

    Param√®tres :
    - prompt (str) : texte √† envoyer au mod√®le

    Retour :
    - str : r√©ponse g√©n√©r√©e par le mod√®le ou message d'erreur
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt}
    ]

    model_name = os.getenv("LLM_RAG")

    try:
        response = client.chat(
            model=model_name,
            messages=messages,
            stream=False,
        )
    except Exception as e:
        return (
            f"‚ö†Ô∏è Erreur de connexion au mod√®le ({model_name}).\n"
            f"Assurez-vous que Ollama est en cours d'ex√©cution.\n\n"
            f"D√©tails : {e}"
        )

    # -----------------------------
    # ‚úÖ Extraction de la r√©ponse de l'assistant selon le format du client Ollama
    # -----------------------------
    # Nouveau format Python Ollama client
    try:
        return response.message.content
    except:
        pass

    # Ancien format (dict)
    try:
        return response["message"]["content"]
    except:
        pass

    # Fallback (debug)
    return str(response)
